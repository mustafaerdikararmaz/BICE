{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mustafaerdikararmaz/BICE/blob/main/bice_week6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJsckvQGjhYq"
      },
      "source": [
        "# Week 6: Plasticity\n",
        "## WANTED: Third Factor\n",
        "We have made time our (dimensional) ally and biology an acquaintance. To truly win over the latter, Alcuin of York (735-804, computational neuroscientist) suggests a show of good faith. On top of biological neuron models, we shall look at how we learn - away from backpropagation, towards biological plausibility! Surely this will not cost us any performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ-Gf3FHm4bR"
      },
      "source": [
        "## STDP for thee (6 points)\n",
        "One popular experiment to visualise the effects of STDP is latency reduction as described in [(Song et al., 2000)](https://www.nature.com/articles/nn0900_919). We will setup a network of several input neurons and a single output, then record spike activity on the post-synaptic one before and after the plasticity phase as well as weight distribution.\n",
        "\n",
        "We won't actually simulate the pre-synaptic neurons; instead we'll generate their spike times in bursts offset by a relative latency to fixed event times. We then feed those pre-generated spike times into the post-synaptic LIF neuron and update the weights for every connection.\n",
        "\n",
        "\n",
        "In the absence of spikes, pre and post synaptic traces decay exponentially:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\tau_{+}\\dot{\\alpha_{pre}} = -\\alpha_{pre} \\\\\n",
        "   \\tau_{-}\\dot{\\alpha_{post}} = -\\alpha_{post}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "When a pre-synaptic spike arrives:\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\alpha_{pre} = \\alpha_{pre} + A_{+} \\\\\n",
        "   w = w + \\alpha_{post}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "conversely, when a post-synaptic spike arrives:\n",
        "$$\n",
        "\\begin{align*}\n",
        "  \\alpha_{post} = \\alpha_{post} + A_{-} \\\\\n",
        "   w = w + \\alpha_{pre}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Below is the basic LIF scaffolding and frequency event generation code.\n",
        "\n",
        "\n",
        "**Exercise 1.1** Fill out the `TODO`s!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_PeKLhL3jba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Basic LIF\n",
        "tau_m = 10.0  # Membrane time constant (ms)\n",
        "R_m = 1.0  # Resistance (MÎ©)\n",
        "V_reset = -75.0  # Reset potential (mV)\n",
        "V_th_base = -55.0  # Spike threshold (mV)\n",
        "V_rest = -70.0  # Resting potential (mV)\n",
        "t_ref = 2  # Refractory period (ms)\n",
        "\n",
        "# Network\n",
        "num_neurons_pre = 1000\n",
        "num_neurons_post = 1\n",
        "\n",
        "# STDP Params\n",
        "tau_plus = 20.0  # Pre-synaptic trace decay const (ms)\n",
        "tau_minus = 5.0  # Post-synaptic trace decay const (ms)\n",
        "A_plus = 0.2  # LTP learning rate\n",
        "A_minus = -0.2  # LTD learning rate (both large values to clearly show the effect)\n",
        "pre_inc = 5.0  # Increment for pre-synaptic trace\n",
        "post_inc = 5.0  # Increment for post-synaptic trace\n",
        "w_max = 15.0 # Maximum weight\n",
        "w_min = 0.0 # Minimum weight\n",
        "\n",
        "\n",
        "def dVm_dt(V, I_syn):\n",
        "    return (-(V - V_rest) + R_m * I_syn) / tau_m\n",
        "\n",
        "\n",
        "def relative_freq_spike_times(sim_time, dt, intra_burst_rate, burst_duration):\n",
        "    event_interval = 250  # arbitrary interval\n",
        "    event_times = np.arange(event_interval, sim_time, event_interval)\n",
        "    relative_latencies = np.random.normal(loc=0, scale=15, size=num_neurons_pre)  # ms\n",
        "\n",
        "    intra_burst_period_ms = (1 / intra_burst_rate) * 1000\n",
        "    intra_burst_period_steps = int(np.round(intra_burst_period_ms / dt))\n",
        "    burst_duration_steps = int(burst_duration / dt)\n",
        "    spike_trains = np.zeros((num_neurons_pre, num_steps))\n",
        "\n",
        "    for event_time_ms in event_times:\n",
        "        for neuron_idx in range(num_neurons_pre):\n",
        "            burst_start_time = event_time_ms + relative_latencies[neuron_idx]\n",
        "            if burst_start_time < 0 or burst_start_time >= sim_time:\n",
        "                continue\n",
        "            burst_start_step = int(np.round(burst_start_time / dt))\n",
        "            burst_end_step = min(burst_start_step + burst_duration_steps, num_steps)\n",
        "            actual_spike_times_in_burst = np.arange(burst_start_step, burst_end_step, intra_burst_period_steps)\n",
        "            valid_spike_indices = actual_spike_times_in_burst[actual_spike_times_in_burst < num_steps].astype(int)\n",
        "            spike_trains[neuron_idx, valid_spike_indices] = 1\n",
        "\n",
        "    return spike_trains\n",
        "\n",
        "\n",
        "def simulate_lif(sim_time, dt, spike_trains):\n",
        "    steps = int(sim_time / dt)\n",
        "\n",
        "    V_m = V_rest\n",
        "    refrac_counter = 0\n",
        "\n",
        "    # Store traces for later plotting\n",
        "    Vm_trace = np.zeros(steps)\n",
        "    Vm_trace[0] = V_m\n",
        "\n",
        "    trace_post = np.zeros(num_neurons_post)  # we only have a single neuron here; you can use trace_post[0] to access it\n",
        "    trace_pre = np.zeros(num_neurons_pre)\n",
        "\n",
        "    initial_weights = np.ones(num_neurons_pre) * (0.25 * w_max)  # static initialisation for all weights\n",
        "    weights = np.copy(initial_weights)\n",
        "\n",
        "    # --- Simulation Loop (Forward Euler Method) ---\n",
        "    for i in range(1, steps):\n",
        "        current_Vm = Vm_trace[i - 1]\n",
        "\n",
        "        spiked_pre = np.where(spike_trains[:, i] == 1)[0]  # all neuron ids that spiked this time step\n",
        "        current_I_syn = np.sum(weights[spiked_pre])  # combined weights of all incoming spikes for current step\n",
        "\n",
        "        # TODO: Exponential decay for traces\n",
        "        trace_pre = ...\n",
        "        trace_post = ...\n",
        "\n",
        "        # TODO: Implement pre-synaptic trace + weights update\n",
        "\n",
        "        dVm = dVm_dt(current_Vm, current_I_syn)  # no background current this time; only synaptic input\n",
        "\n",
        "        updated_Vm = current_Vm\n",
        "        if refrac_counter == 0:\n",
        "            updated_Vm = current_Vm + dVm * dt\n",
        "        else:\n",
        "            refrac_counter -= 1\n",
        "\n",
        "        if updated_Vm >= V_th_base:\n",
        "            updated_Vm = V_reset\n",
        "            refrac_counter = int(t_ref / dt)\n",
        "\n",
        "            # TODO: Implement post-synaptic trace + weights update\n",
        "\n",
        "        Vm_trace[i] = updated_Vm\n",
        "\n",
        "    return Vm_trace, initial_weights, weights\n",
        "\n",
        "\n",
        "def plot_weight_dist(initial_weights, final_weights):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.hist(initial_weights, bins=50, alpha=0.7, label='Initial Weights', color='blue', range=(w_min, w_max))\n",
        "    plt.hist(final_weights, bins=50, alpha=0.7, label='Final Weights', color='red', range=(w_min, w_max))\n",
        "\n",
        "    plt.xlabel(\"Weight Value\")\n",
        "    plt.ylabel(\"Number of Synapses\")\n",
        "    plt.title(\"Weight Distribution Before and After STDP\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sim_duration = 10000.0  # Total simulation time (ms)\n",
        "    time_step = 0.1  # Time step (ms)\n",
        "    num_steps = int(sim_duration / time_step)\n",
        "\n",
        "    # Run & plot\n",
        "    spike_trains = relative_freq_spike_times(sim_duration, time_step, intra_burst_rate=100, burst_duration=20)\n",
        "    voltage_trace, initial_weights, final_weights = simulate_lif(sim_duration, time_step, spike_trains)\n",
        "    plot_weight_dist(initial_weights, final_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1.2** Answer the following questions (experimentally if you want, you can edit the answers in this cell on your notebook copy):\n",
        "\n",
        "1. We should be able to observe a very bimodal distribution of weights. What is the relation between those strengthened to $w_{max}$ and the relative frequency of the corresponding pre-synaptic neurons?\n",
        "\n",
        "1. Given default values, do you expect the spike rate of the post-synaptic neuron to be higher or lower towards the end of the simulation compared to start? Why?\n",
        "\n",
        "3. How do you think (vanilla) STDP as a learning rule would perform on more complex benchmark tasks (e.g. MNIST images from previous exercises)? Explain your reasoning."
      ],
      "metadata": {
        "id": "vSEoQmiFzwhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local learning rules (4 points)\n",
        "Let's start with a simple point neuron\n",
        "$$\n",
        "y=\\sum_{i=1}^n w_ix_i.\n",
        "$$\n",
        "We want to learn the weights $w_i$ locally, i.e. as a function of\n",
        "- a global **modulator** $\\alpha$\n",
        "- the **pre-synaptic activation** $x_i$\n",
        "- the **post-synaptic activation** $y$.\n",
        "\n",
        "You have already learned about the **Hebbian learning rule**: as a novel input $(x_1,\\ldots,x_d)$ arrives, we update the weights as\n",
        "$$\n",
        "w_i\\leftarrow w_i + \\alpha x_iy.\n",
        "$$\n",
        "\n",
        "However, assume that for some synapse $i$, $x_iy>0$ for all incoming inputs, and the global neuromodulator stays constant: the synaptic weight will eventually explode.\n",
        "\n",
        "Therefore, **Oja's learning rule** instead suggests \"punishing\" the update term by the magnitude of the synaptic weight itself.\n",
        "\n",
        "$$\n",
        "w_i\\leftarrow w_i + \\alpha (x_iy-y^2w_i).\n",
        "$$\n",
        "\n",
        "For stability, we project $\\mathbf{w}$ to the unit sphere after each iteration.\n",
        "Your task will be to implement Oja's rule."
      ],
      "metadata": {
        "id": "TJCGn5ne3pv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "MyWCuiJb4WKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2.1**: Generate 10,000 points sampled from a Gaussian distribution with\n",
        "$$\n",
        "\\mu=\\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} \\qquad\n",
        "\\Sigma=\\begin{pmatrix}1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1\\end{pmatrix}\n",
        "$$\n",
        "as a float32 `torch.Tensor` called `points` of shape `(10000, 3)`."
      ],
      "metadata": {
        "id": "YrM4zNU14cdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n"
      ],
      "metadata": {
        "id": "fb5MiUaZ4j9D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "57d28331-46e9-4ba7-a0c7-57f4ee5deb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c522e71cdbcb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO REVIEW + it literally says \"kursiver text\" below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code to plot your point cloud.\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(points[:,0], points[:,1], points[:,2])\n",
        "\n",
        "# Plot stuff\n",
        "ax.set_xlabel('X')\n",
        "ax.set_xlabel('Y')\n",
        "ax.set_xlabel('Z')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_lsKxaiI4l9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2.2**: Implement Oja's rule for the generated point cloud.\n",
        "- Initialize the weight matrix from a standard Gaussian.\n",
        "- For each point, the target is defined as\n",
        "$$y=\\mathbf{w}^T\\mathbf{x}.$$\n",
        "- For timestep $t$, the global modulator is $$\\alpha(t)=1/t,$$ i.e. it decreases over time.\n",
        "- Don't forget to project the weight vector on the unit sphere after each iteration ($\\ell$2-normalization).\n",
        "- print the final w (you may print intermediary steps)"
      ],
      "metadata": {
        "id": "HlATPUeX4tXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n"
      ],
      "metadata": {
        "id": "o53TS-bs5N3V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "9dc0978f-e9ea-4e1e-e68e-decb9a98649f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-672bf5125a4a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2.3**: Use `torch.linalg.svd()` to compute the principal components of your point cloud. What do you observe?"
      ],
      "metadata": {
        "id": "TXzLtXFENSz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n"
      ],
      "metadata": {
        "id": "M48EDIsXNaWh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}